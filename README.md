# ğŸ¤– AI Chatbot with Redis Memory & Smart Session Handling

A production-style AI chatbot built with Groq LLM and Redis-based smart caching.  
Designed to optimize token usage, reduce API cost, and maintain isolated user sessions with TTL-based memory management.

---

## ğŸš€ Project Overview

This project implements a Generative AI chatbot with:

- ğŸ” Cache-based memory architecture
- ğŸ§  Conversation history stored as summaries
- â³ TTL-based auto-expiry using Redis
- ğŸ‘¤ Session-based user isolation
- âš¡ Smart repeat question detection
- ğŸ’¬ Minimal SaaS-style UI (Streamlit)

Instead of storing full conversations, the system stores compact summaries to ensure efficient memory handling and lower storage cost.

---

## ğŸ§© How It Works

### ğŸ” Authentication
- User logs in with a unique username
- Each username acts as a Session ID

### ğŸ§  First-Time Question
- Full answer generated via Groq LLM
- Short summary created from full response
- Summary stored in Redis with TTL
- â€œSeenâ€ flag saved for repeat detection

### ğŸ” Repeat Question
- System detects same query using hash
- Returns cached summary instantly
- No new LLM API call
- Saves tokens & reduces latency

### ğŸ“‚ Session Reuse
- If same Session ID (username) is used
- User can access previous conversation summaries

---

## ğŸ— Redis Architecture

cache:{user}:summary:{hash}
cache:{user}:seen:{hash}
history:{user}

- ğŸ”‘ Query hash ensures repeat detection
- â³ TTL auto-cleans inactive cache
- ğŸ“Š Sorted history using timestamps

---

## âš™ Tech Stack

- **Groq LLM (Llama 3.3)**
- **Redis**
- **Streamlit**
- **Python**
- **TTL-based caching**
- **Hash-based query matching**

